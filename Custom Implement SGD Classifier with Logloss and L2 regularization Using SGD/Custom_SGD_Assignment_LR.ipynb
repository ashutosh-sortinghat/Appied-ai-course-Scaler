{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfe2NTQtLq11"
   },
   "source": [
    "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt \n",
    "import pdb\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.50.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# please don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "# make_classification is used to create custom dataset \n",
    "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#please don't change random state\n",
    "# you need not standardize the data as it is already standardized\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.77, NNZs: 15, Bias: -0.316653, T: 37500, Avg. loss: 0.455552\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.91, NNZs: 15, Bias: -0.472747, T: 75000, Avg. loss: 0.394686\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.98, NNZs: 15, Bias: -0.580082, T: 112500, Avg. loss: 0.385711\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.02, NNZs: 15, Bias: -0.658292, T: 150000, Avg. loss: 0.382083\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.719528, T: 187500, Avg. loss: 0.380486\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.05, NNZs: 15, Bias: -0.763409, T: 225000, Avg. loss: 0.379578\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.795106, T: 262500, Avg. loss: 0.379150\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.819925, T: 300000, Avg. loss: 0.378856\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.07, NNZs: 15, Bias: -0.837805, T: 337500, Avg. loss: 0.378585\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.08, NNZs: 15, Bias: -0.853138, T: 375000, Avg. loss: 0.378630\n",
      "Total training time: 0.09 seconds.\n",
      "Convergence after 10 epochs took 0.09 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.42336692,  0.18547565, -0.14859036,  0.34144407, -0.2081867 ,\n",
       "          0.56016579, -0.45242483, -0.09408813,  0.2092732 ,  0.18084126,\n",
       "          0.19705191,  0.00421916, -0.0796037 ,  0.33852802,  0.02266721]]),\n",
       " (1, 15),\n",
       " array([-0.8531383]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1_8bdzitDlM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.  We will be giving you some functions, please write code in that functions only.\n",
    "\n",
    "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
    "\n",
    "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
    "\n",
    "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
    "\n",
    "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
    "\n",
    "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
    "\n",
    "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
    "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
    "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
    "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
    "        you can stop the training\n",
    "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    row_vector = np.arange(dim)\n",
    "    w = np.zeros_like(row_vector)\n",
    "    b = 0\n",
    "    #initialize the weights as 1d array consisting of all zeros similar to the dimensions of row_vector\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [],
   "source": [
    "# # checking function - how its working\n",
    "# dim=len(X_train[0]) \n",
    "# w,b = initialize_weights(dim)\n",
    "# print('w =',(w))\n",
    "# print('b =',str(b))\n",
    "# w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MI5SAjP9ofN"
   },
   "source": [
    "<font color='red'>Grader function - 1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Pv1llH429wG5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(len(dim))\n",
    "def grader_weights(w,b):\n",
    "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "  return True\n",
    "    \n",
    "grader_weights(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z,b=0):\n",
    "    \n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    prob = 1/(1 + math.exp(-z))  \n",
    "    # compute sigmoid(z) and return\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YrGDwg3Ae4m"
   },
   "source": [
    "\n",
    "<font color='red'>Grader function - 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "P_JASp_NAfK_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):  \n",
    "  val=sigmoid(z)\n",
    "  assert(val==0.8807970779778823)\n",
    "  return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predicting yi from sigmoid function in order to check sigmoid()\n",
    "# import pdb\n",
    "# y_pred = []\n",
    "# for x,y in zip(X_train,y_train):\n",
    "#     z = y*np.dot(w,x)+ b\n",
    "#     yi = sigmoid(z)  # return probability\n",
    "# #     pdb.set_trace() \n",
    "#     y_pred.append(yi)\n",
    "#     z=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"y_pred :\",len(y_pred),\"y_true :\",len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "def logloss(y_true,yi_pred):\n",
    "    # you have been given two arrays y_true and y_pred and you have to calculate the logloss\n",
    "       \n",
    "    #while dealing with numpy arrays you can use vectorized operations for quicker calculations as compared to using loops\n",
    "    #https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html\n",
    "    #https://www.geeksforgeeks.org/vectorized-operations-in-numpy/    \n",
    "    #write your code here\n",
    "    # performing sum, log to all matrix through numpy     \n",
    "    n = len(y_true)\n",
    "    loss =  -1/n * np.sum( np.sum( y_true * np.log10(yi_pred)) + np.sum((1-y_true)*np.log10(1-yi_pred))) \n",
    "#     pdb.set_trace()\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs1BTXVSClBt"
   },
   "source": [
    "<font color='red'>Grader function - 3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LzttjvBFCuQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#round off the value to 8 values\n",
    "def grader_logloss(true,pred):\n",
    "  loss=logloss(true,pred)\n",
    "#   print(loss)  \n",
    "  assert(np.round(loss,6)==0.076449)\n",
    "  return True\n",
    "true=np.array([1,1,0,1,0])\n",
    "pred=np.array([0.9,0.8,0.1,0.8,0.2])\n",
    "grader_logloss(true,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQabIadLCBAB"
   },
   "source": [
    "**loss funtion**\n",
    "\n",
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "\n",
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br><br>\n",
    "\n",
    "**above function is derivation of loss function wrt to w**\n",
    "\n",
    "**Explanation**<br>\n",
    "we have above formula for computing vector gradient/deffirential for a single data_point(vector).<br>\n",
    "(means- w and x both are vector)<br>\n",
    "formula we get - after differentiating formula of LR wrt w.<br>\n",
    "to find gradient - we are putting value of x_i(where i goes from 1 to n (or k)) \n",
    "and W (where both are vectors), and the result we got is gradient of f()(= LR) w.r.t. w for a single pt x (is vector)  \n",
    "and the result we get is called \"differentiation wrt w\" is also vector.<br>\n",
    "to find more - go for lec-30.1,module-3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "\n",
    "#make sure that the sigmoid function returns a scalar value, you can use dot function operation\n",
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    \n",
    "    z = np.dot(w,x)+ b\n",
    "    dw = np.dot(x,(y-sigmoid(z)))-np.dot(alpha/N,w)\n",
    "    pdb.set_trace()    \n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUFLNqL_GER9"
   },
   "source": [
    "<font color='red'>Grader function - 4 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WI3xD8ctGEnJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> \n",
      " sum  4.7568393878789825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "  print(type(grad_dw),\"\\n sum\",\"\",np.sum(grad_dw))  \n",
    "  assert(np.round(np.sum(grad_dw),5)==4.75684)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w=np.array([ 0.03364887,  0.03612727,  0.02786927,  0.08547455, -0.12870234,\n",
    "       -0.02555288,  0.11858013,  0.13305576,  0.07310204,  0.15149245,\n",
    "       -0.05708987, -0.064768  ,  0.18012332, -0.16880843, -0.27079877])\n",
    "grad_b=0.5\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE8g84_GI62n"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    "#sb should be a scalar value\n",
    "def gradient_db(x,y,w,b):\n",
    "     '''In this function, we will compute gradient w.r.to b '''\n",
    "     z = np.dot(w,x)+ b\n",
    "     db =  y - sigmoid(z)      \n",
    "     return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbcBzufVG6qk"
   },
   "source": [
    "<font color='red'>Grader function - 5 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "  grad_db=gradient_db(x,y,w,b)\n",
    "  assert(np.round(grad_db,4)==-0.3714)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0.5\n",
    "grad_b=0.1\n",
    "grad_w=np.array([ 0.03364887,  0.03612727,  0.02786927,  0.08547455, -0.12870234,\n",
    "       -0.02555288,  0.11858013,  0.13305576,  0.07310204,  0.15149245,\n",
    "       -0.05708987, -0.064768  ,  0.18012332, -0.16880843, -0.27079877])\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function used to compute predicted_y given the dataset X\n",
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "#         pdb.set_trace()\n",
    "        z=np.dot(w,X[i])+b\n",
    "        predict.append(sigmoid(z))\n",
    "    return np.array(predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0,verbose):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    #implement the code as follows\n",
    "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    w,b = initialize_weights(len(X_train[0])) #passing dimention as parameter\n",
    "#     pdb.set_trace()\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    N = len(y_train)\n",
    "    loss = 0   \n",
    "    # for every epoch\n",
    "    for e in range(10):\n",
    "        # for every data point(X_train,y_train) using mini-batch SGD with batch size =1 ptn  \n",
    "        for i in range(N):\n",
    "#             pdb.set_trace()\n",
    "\n",
    "            #compute gradient w.r.to w (call the gradient_dw(x,y,w,b,alpha,N) function)\n",
    "            dw = gradient_dw(X_train[i],y_train[i],w,b,alpha,N)\n",
    "            #compute gradient w.r.to b (call the gradient_db(x,y,w,b) function)\n",
    "            db = gradient_db(X_train[i],y_train[i],w,b)\n",
    "           #update w, b\n",
    "            w = np.add(w, alpha*dw)\n",
    "            b = np.add(b, alpha*db)\n",
    "        # predict the output of x_train [for all data points in X_train] using pred function with updated weights\n",
    "        \n",
    "        y_pred = pred(w,b,X_train)\n",
    "        #compute the loss between predicted and actual values (call the loss function- logloss(y_true,yi_pred))\n",
    "        loss1 = logloss(y_train,y_pred)\n",
    "        # store all the train loss values in a list\n",
    "        train_loss.append(loss1)\n",
    "        # predict the output of x_test [for all data points in X_test] using pred function with updated weights\n",
    "        test_y_pred = pred(w,b,X_test)\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        loss2 = logloss(y_test,test_y_pred)\n",
    "        # store all the test loss values in a list\n",
    "        test_loss.append(loss2)\n",
    "        if verbose == 1 : print(\"epoch no: \",e,\"train loss \",loss1,\"test loss\",loss2); \n",
    "        \n",
    "        # returning values if loss is not updating  \n",
    "#         diff_train_loss = train_loss[e]-train_loss[e-1]\n",
    "#         if diff_train_loss <= .000001:\n",
    "#             return w,b,train_loss,test_loss,e\n",
    "            \n",
    "          \n",
    "\n",
    "    #write your code to perform SGD\n",
    "\n",
    "\n",
    "    return w,b,train_loss,test_loss,epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "id": "sUquz7LFEZ6E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-33-a88ed98774ca>\u001b[0m(8)\u001b[0;36mgradient_dw\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m      4 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      5 \u001b[1;33m    \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      6 \u001b[1;33m    \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      7 \u001b[1;33m    \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m----> 8 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> x.shape\n",
      "(15,)\n",
      "ipdb> w.shape\n",
      "*** invalid literal for int() with base 10: '.shape'\n",
      "ipdb> p w\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "ipdb> w\n",
      "\u001b[1;31m    [... skipping 27 hidden frame(s)]\u001b[0m\n",
      "\n",
      "  \u001b[1;32m<ipython-input-37-5f958804b636>\u001b[0m(5)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      2 \u001b[0m\u001b[0meta0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      3 \u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      4 \u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m----> 5 \u001b[1;33m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meta0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  \u001b[1;32m<ipython-input-36-52213439f216>\u001b[0m(20)\u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     18 \u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     19 \u001b[0m            \u001b[1;31m#compute gradient w.r.to w (call the gradient_dw(x,y,w,b,alpha,N) function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 20 \u001b[1;33m            \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_dw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0m            \u001b[1;31m#compute gradient w.r.to b (call the gradient_db(x,y,w,b) function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     22 \u001b[0m            \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "> \u001b[1;32m<ipython-input-33-a88ed98774ca>\u001b[0m(8)\u001b[0;36mgradient_dw\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m      4 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      5 \u001b[1;33m    \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      6 \u001b[1;33m    \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      7 \u001b[1;33m    \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m----> 8 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> sigmoid(z)\n",
      "0.5\n",
      "ipdb> exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-5f958804b636>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meta0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-52213439f216>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X_train, y_train, X_test, y_test, epochs, alpha, eta0, verbose)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m#compute gradient w.r.to w (call the gradient_dw(x,y,w,b,alpha,N) function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_dw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;31m#compute gradient w.r.to b (call the gradient_db(x,y,w,b) function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-a88ed98774ca>\u001b[0m in \u001b[0;36mgradient_dw\u001b[1;34m(x, y, w, b, alpha, N)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-a88ed98774ca>\u001b[0m in \u001b[0;36mgradient_dw\u001b[1;34m(x, y, w, b, alpha, N)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha=0.001\n",
    "eta0=0.001\n",
    "N=len(X_train)\n",
    "epochs=10\n",
    "w,b,train_loss,test_loss,e = train(X_train,y_train,X_test,y_test,epochs,alpha,eta0,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "## <font color='red'>Goal of assignment</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in order of 10^-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Grader function - 6 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The custom weights are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this grader function should return True\n",
    "#the difference between custom weights and clf.coef_ should be less than or equal to 0.05\n",
    "def differece_check_grader(w,b,coef,intercept):\n",
    "    val_array=np.abs(np.array(w-coef))\n",
    "    assert(np.all(val_array<=0.05))\n",
    "    print('The custom weights are correct')\n",
    "    return True\n",
    "differece_check_grader(w,b,clf.coef_,clf.intercept_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot your train and test loss vs epochs </font>\n",
    "\n",
    "plot epoch number on X-axis and loss on Y-axis and make sure that the curve is converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1O6GrRt7UeCJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "num = [i for i in range(1,11)]\n",
    "plt.scatter(num, test_loss)\n",
    "plt.plot(num, test_loss, label=\"test_loss\")\n",
    "plt.plot(num, train_loss, label=\"train_loss\")\n",
    "plt.scatter(num, train_loss )\n",
    "plt.legend()\n",
    "plt.title('train loss and test loss per epoch')\n",
    "plt.show()\n",
    "# print(train_loss,\"/n\",num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "FUN8puFoEZtU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 20 feature\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-k28U1xDsLIO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMokBfs3-2PY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
